{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import os\n",
    "import warnings\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from abc import ABC\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from transformers import BertForTokenClassification, BertConfig, Adafactor, AdamW\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
    "import torch.nn.functional as F\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import label_ranking_average_precision_score, average_precision_score, f1_score\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "from datasets import load_from_disk\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "from Bio import SeqIO #parse fasta files\n",
    "import os #change directories\n",
    "import time #track processing time\n",
    "import pandas as pd #create dataframes\n",
    "from datasets import load_dataset #load dataframe into huggingface dataset\n",
    "from sklearn import preprocessing #encode labels\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class TokenDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in\n",
    "                self.encodings.items()}  # keys are input_ids, token_type_ids, attention_mask, labels, values are stored as a list of lists\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))\n",
    "\n",
    "\n",
    "class newDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.encodings = {\"input_ids\": dataframe[\"input_ids\"], \"token_type_ids\": dataframe[\"token_type_ids\"],\n",
    "                          \"attention_mask\": dataframe[\"attention_mask\"]}\n",
    "        self.labels = {\"labels\": dataframe[\"labels\"]}\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\"input_ids\": self.encodings[\"input_ids\"][idx], \"token_type_ids\": self.encodings[\"token_type_ids\"][idx],\n",
    "                \"attention_mask\": self.encodings[\"attention_mask\"][idx], \"labels\": self.labels[\"labels\"][idx]}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels[\"labels\"]))\n",
    "\n",
    "class BertTokClassification(pl.LightningModule, ABC):\n",
    "    def __init__(\n",
    "            self,\n",
    "            config: BertConfig = None,\n",
    "            pretrained_dir: str = None,\n",
    "            use_adafactor: bool = False,\n",
    "            learning_rate=3e-5,\n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.use_adafactor = use_adafactor\n",
    "        if pretrained_dir is None:\n",
    "            self.bert = BertForTokenClassification(config, **kwargs)\n",
    "        else:\n",
    "            self.bert = BertForTokenClassification.from_pretrained(pretrained_dir, **kwargs)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        return self.bert(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        outputs = self(input_ids=input_ids.to(self.device), attention_mask=attention_mask.to(self.device), labels=labels.to(self.device, dtype=torch.int64))\n",
    "        loss = outputs.loss\n",
    "\n",
    "\n",
    "        def get_acc(labels, logits):\n",
    "            sumList = []\n",
    "            for i in range(len(labels)):\n",
    "                y_pred = torch.max(logits[i], 1).indices\n",
    "                score = accuracy_score(labels[i], y_pred)\n",
    "                sumList.append(score)\n",
    "            avg = sum(sumList) / len(labels)\n",
    "            return avg\n",
    "\n",
    "\n",
    "        accuracy1 = get_acc(labels.cpu(), outputs.logits.cpu())\n",
    "\n",
    "        # accuracy = balanced_accuracy_score(master[0], master[1])\n",
    "        self.log(\n",
    "            \"train_batch_accuracy\",\n",
    "            accuracy1,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        outputs = self(input_ids=input_ids.to(self.device), attention_mask=attention_mask.to(self.device), labels=labels.to(self.device, dtype=torch.int64))\n",
    "        loss = outputs.loss\n",
    "        self.log(\n",
    "            \"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "\n",
    "        # def get_balanced_accuracy(labels, logits):\n",
    "        #     y_pred = torch.max(logits, 1).indices\n",
    "        #     score = balanced_accuracy_score(labels, y_pred)\n",
    "        #     return score\n",
    "        #\n",
    "        # def label_average_precision(labels, logits):\n",
    "        #     y_pred = torch.max(prob, 1).indices\n",
    "        #     score = label_ranking_average_precision_score(labels, y_pred)\n",
    "        #     return score\n",
    "        # def f1_calc(labels, logits):\n",
    "        #     sumList = []\n",
    "        #     for i in range(len(labels)):\n",
    "        #         y_pred = torch.max(logits[i], 1).indices\n",
    "        #         score = f1_score(labels[i], y_pred, average='macro')\n",
    "        #         sumList.append(score)\n",
    "        #     avg = sum(sumList) / len(labels)\n",
    "        #     return avg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # \"\"\"\n",
    "        # 1. Iterate over the batch:\n",
    "        #     for each_label in labels.cpu():\n",
    "        #         shorten the length of the list to its true length using attention list and the function [true_length]\n",
    "        #\n",
    "        #     for each_logit in outputs.logits.cpu():\n",
    "        #         use torch.max(outputs.logits.cpu()[0], 1) to get the indices for each logit (best label prediction)\n",
    "        #         shorten the indices to its proper label length\n",
    "        #         compare the indices to the labels\n",
    "        # \"\"\"\n",
    "        def true_length(y_attention_mask):  # finds the start and stop of the actual sequence\n",
    "            switch = False\n",
    "            start = 0\n",
    "            stop = 0\n",
    "            counter = 0\n",
    "            attention_mask = list(y_attention_mask)\n",
    "            for i in attention_mask:\n",
    "                if int(i) == 1 and switch == False:\n",
    "                    switch = True\n",
    "                    start = counter\n",
    "                elif int(i) == 0 and switch == True:\n",
    "                    stop = counter\n",
    "                    break\n",
    "                elif counter == 511:\n",
    "                    stop = 512\n",
    "                counter += 1\n",
    "            return (start, stop)\n",
    "\n",
    "\n",
    "        def short_clean(attention_mask, labels, logits): #attention_mask, labels.cpu(), outputs.logits.cpu()\n",
    "\n",
    "            def true_length(y_attention_mask):  # finds the start and stop of the actual sequence\n",
    "                switch = False\n",
    "                start = 0\n",
    "                stop = 0\n",
    "                counter = 0\n",
    "                attention_mask = list(y_attention_mask)\n",
    "                for i in attention_mask:\n",
    "                    if int(i) == 1 and switch == False:\n",
    "                        switch = True\n",
    "                        start = counter\n",
    "                    elif int(i) == 0 and switch == True:\n",
    "                        stop = counter\n",
    "                        break\n",
    "                    counter += 1\n",
    "                return (start, stop)\n",
    "\n",
    "            masterPred = []\n",
    "            masterTrue = []\n",
    "            for batch_index in range(len(labels)):\n",
    "                real_len = true_length(attention_mask[batch_index])\n",
    "                predIndecies = torch.max(outputs.logits.cpu()[batch_index], 1).indices\n",
    "                start = real_len[0]\n",
    "                stop = real_len[1]\n",
    "                currentTrue = torch.LongTensor(labels[batch_index][start:stop])\n",
    "                currentPred = torch.LongTensor(predIndecies[start:stop])\n",
    "                if len(currentTrue) == 0:\n",
    "                    masterTrue.append(currentTrue.tolist())\n",
    "                    masterPred.append(currentPred.tolist())\n",
    "                    print(f\"CURRENT-PRED LEN: {len(currentPred)}\")\n",
    "                    print(f\"CURRENT-TRUE LEN:{len(currentTrue)}\")\n",
    "\n",
    "            return (masterTrue, masterPred)\n",
    "\n",
    "        master = short_clean(attention_mask, labels.cpu(), outputs.logits.cpu())\n",
    "        print(\"###################################\")\n",
    "        print(f\"MASTER-TRUE: {master[0]}\")\n",
    "        print(\"###################################\")\n",
    "        print(f\"MASTER-PRED: {master[1]}\")\n",
    "        print(\"###################################\")\n",
    "        print(\"=======\")\n",
    "        print(f\"LABEL LEN: {len(labels.cpu())}\")\n",
    "        for i in range(len(labels.cpu())):\n",
    "            print(f\"SINGLE LABEL LEN: {len(labels.cpu()[i])}\")\n",
    "            print(f\"ATTENTION LEN: {len(attention_mask[i])}\")\n",
    "            #print(attention_mask[i])\n",
    "            print(f\"TRUE LEN: {true_length(attention_mask[i])}\")\n",
    "        print(\"=======\")\n",
    "        print(f\"LABELS: {labels.cpu()}\")\n",
    "        print(\"||||||||||||||||||||||||||||\")\n",
    "        print(\"=======\")\n",
    "        print(f\"LOGITS LEN: {len(outputs.logits.cpu())}\")\n",
    "        for i in outputs.logits.cpu():\n",
    "            print(f\"SINGLE LOGIT LEN: {len(i)}\")\n",
    "        print(f\"LOGIT SINGLE LIST LEN: {len(outputs.logits.cpu()[0][0])}\")\n",
    "        b_logit = torch.max(outputs.logits.cpu()[0], 1)\n",
    "        b_logit_indices = torch.max(outputs.logits.cpu()[0], 1).indices\n",
    "        print(f\"LOGIT BEST: {b_logit}\")\n",
    "        print(f\"LOGIT BEST INDICES: {b_logit_indices}\")\n",
    "        print(\"=======\")\n",
    "        print(f\"LOGITS: {outputs.logits.cpu()}\")\n",
    "\n",
    "        # accuracy = label_average_precision(labels.cpu(), logits=outputs.logits.cpu()) #replaced get_balanced_accuracy(labels.cpu(), logits=outputs.logits.cpu()) with label ranking average precision\n",
    "\n",
    "        # def balanced_accuracy_score(labels, logits):\n",
    "        #     sumList = []\n",
    "        #     for i in range(len(labels)):\n",
    "        #         y_predList = []\n",
    "        #         trueList = []\n",
    "        #         y_pred = logits[i]\n",
    "        #         previous = 0\n",
    "        #         for lab in labels[i]:\n",
    "        #             if lab == -100:\n",
    "        #                 y_predList.append(y_pred[previous])\n",
    "        #                 trueList.append(previous)\n",
    "        #             else:\n",
    "        #                 previous = lab\n",
    "        #                 y_predList.append(y_pred[previous])\n",
    "        #                 trueList.append(previous)\n",
    "        #\n",
    "        #         num = average_precision_score(trueList, y_predList)\n",
    "        #         sumList.append(num)\n",
    "        #     big = sum(sumList) / len(labels)\n",
    "        #     return big\n",
    "\n",
    "        def get_bal_acc(labels, logits):\n",
    "            sumList = []\n",
    "            for i in range(len(labels)):\n",
    "                y_pred = torch.max(logits[i], 1).indices\n",
    "                score = balanced_accuracy_score(labels[i], y_pred)\n",
    "                sumList.append(score)\n",
    "            avg = sum(sumList) / len(labels)\n",
    "            return avg\n",
    "\n",
    "        accuracy = get_bal_acc(labels.cpu(), outputs.logits.cpu())\n",
    "\n",
    "        self.log(\n",
    "            \"val_accuracy\",\n",
    "            accuracy,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        return {\"val_loss\": loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.use_adafactor:\n",
    "            return Adafactor(\n",
    "                self.parameters(),\n",
    "                lr=self.learning_rate,\n",
    "                eps=(1e-30, 1e-3),\n",
    "                clip_threshold=1.0,\n",
    "                decay_rate=-0.8,\n",
    "                beta1=None,\n",
    "                weight_decay=0.0,\n",
    "                relative_step=False,\n",
    "                scale_parameter=False,\n",
    "                warmup_init=False)\n",
    "        else:\n",
    "            return AdamW(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def save_pretrained(self, pretrained_dir):\n",
    "        self.bert.save_pretrained(self, prtrained_dir)\n",
    "\n",
    "    def predict_classes(self, input_ids, attention_mask, return_logits=False):\n",
    "        output = self.bert(input_ids=input_ids.to(self.device), attention_mask=attention_mask)\n",
    "        if return_logits:\n",
    "            return output.logits\n",
    "        else:\n",
    "            probabilities = F.sigmoid(output.logits)\n",
    "            predictions = torch.argmax(probabilities)\n",
    "            return {\"probabilities\": probabilities, \"predictions\": predictions}\n",
    "\n",
    "    def get_attention(self, input_ids, attention_mask, specific_attention_head: int = None):\n",
    "        output = self.bert(inputs_ids=input_ids.to(self.device), attention_mask=attention_mask)\n",
    "        if specific_attention_head is not None:\n",
    "            last_layer = output.attentions[-1]  # grabs the last layer\n",
    "            all_last_attention_heads = [torch.max(this_input[specific_attention_head], axis=0)[0].indices for this_input in last_layer]\n",
    "            return all_last_attention_heads\n",
    "        return output.attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'sequence', 'labels', 'start', 'stop'], dtype='object')\n",
      "Index(['sequence', 'labels', 'start', 'stop'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "gpu_idx = 0\n",
    "num_labels = 60\n",
    "\n",
    "#model to evaluate with\n",
    "model_path = \"/mnt/storage/grid/home/eric/hmm2bert/models/pullin/pullin>1000_whiteSpace_best_loss-epch9{1GPU}.pt\"\n",
    "#encoded csv to use\n",
    "data_folder = \"pullin_parsed_data\"\n",
    "encoded_label_filename = \"encoded_parsed_pullin_noDupes_whiteSpace>1000_withAA_not_domain.csv\"\n",
    "encoded_csv = f\"/mnt/storage/grid/home/eric/hmm2bert/{data_folder}/{encoded_label_filename}\"\n",
    "####################################################\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"MKL_THREADING_LAYER\"] = \"GNU\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "#load in tokenizer, model (eval mode) and pipeline\n",
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n",
    "model = torch.load(model_path)\n",
    "model.eval()\n",
    "meatpipe = pipeline(task='ner', model=model.bert, tokenizer=tokenizer, device=gpu_idx)\n",
    "\n",
    "# load csv to pandas dataframe\n",
    "df = pd.read_csv(encoded_csv)\n",
    "print(df.columns)\n",
    "df = df.drop([\"Unnamed: 0\"], axis=1)\n",
    "print(df.columns)\n",
    "\n",
    "#split the dataset into train and test, this produces a list with the row positions\n",
    "strat_train, strat_test = train_test_split(df, test_size=.2, stratify=df['labels'], random_state=420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#start count @ 0\n",
    "def labelLabel(label, sequence, start, stop):\n",
    "    sequence = sequence.split()\n",
    "    sequence = \"\".join(sequence)\n",
    "    domainList = list(range(start, stop + 1))\n",
    "    sequenceIndexCounter = 0\n",
    "    trueList = []\n",
    "\n",
    "    for currentNum in range(len(sequence)):  # iterate over each amino acid in a single sequence\n",
    "        # ====\n",
    "        shiftedNum = currentNum  #CHANGED FROM currentNum+1 TO currentNum # index that we compare to as sequence index starts at zero but start/stop starts at 1\n",
    "        # ====\n",
    "        if shiftedNum == domainList[sequenceIndexCounter]:\n",
    "            trueList.append(label)\n",
    "            if domainList[sequenceIndexCounter] == domainList[-1]:\n",
    "                    sequenceIndexCounter = 0\n",
    "\n",
    "            else:\n",
    "                sequenceIndexCounter += 1\n",
    "        else:\n",
    "            trueList.append(0)\n",
    "    return trueList\n",
    "\n",
    "def domainAcc(true_label, predict):  # finds the start and stop of the actual sequence (range style where stop is actually stop - 1)\n",
    "    switch = False\n",
    "    start = None\n",
    "    stop = len(true_label)\n",
    "    counter = 0 #CHANGED FROM 1 TO 0\n",
    "    domainLabel = None\n",
    "    numCorrectLabels = 0\n",
    "    numNotLabels = 0\n",
    "    fullDomainCounter = 0\n",
    "    \n",
    "    #gets accuracy of prediction of whole sequence\n",
    "    for i in range(len(true_label)):\n",
    "        if true_label[i] == predict[i]:\n",
    "            fullDomainCounter += 1\n",
    "    fullDomainScore = fullDomainCounter/len(predict)\n",
    "\n",
    "    #sets the start and stop of domain based on true label list\n",
    "    for i in true_label:\n",
    "        if int(i) != 0 and switch == False:\n",
    "            switch = True\n",
    "            start = counter\n",
    "        elif int(i) == 0 and switch == True:\n",
    "            stop = counter - 1\n",
    "            break\n",
    "        counter += 1\n",
    "    \n",
    "    #if domain is out of range and whole sequence is notDomain set start equal to stop + 1\n",
    "    if start == None:\n",
    "        start = stop + 1\n",
    "    \n",
    "    #calculates the number of amino acids in domain\n",
    "    numInDomain = stop - (start)\n",
    "\n",
    "    for i in true_label:\n",
    "        if i != 0:\n",
    "            domainLabel = i\n",
    "\n",
    "    for i in predict[start - 1:stop]:\n",
    "        if i == domainLabel:\n",
    "            numCorrectLabels += 1\n",
    "    try:\n",
    "        domainLabelAcc = numCorrectLabels / numInDomain\n",
    "    except:\n",
    "        domainLabelAcc = 0\n",
    "        \n",
    "    for i in predict[:start - 1]:\n",
    "        if i == 0:\n",
    "            numNotLabels += 1\n",
    "\n",
    "    for i in predict[stop - 1:]:\n",
    "        if i == 0:\n",
    "            numNotLabels += 1\n",
    "\n",
    "    numNotInDomain = len(true_label) - numInDomain\n",
    "    try:\n",
    "        notDomainLabelAcc = numNotLabels / numNotInDomain\n",
    "    except:\n",
    "        notDomainLabelAcc = 0 #numNotLabels / len(true_label) #maybe change to 0?\n",
    "\n",
    "    return [start, stop, domainLabelAcc, notDomainLabelAcc, fullDomainScore]\n",
    "\n",
    "def matching_labels(true, predict, domainStart, domainStop):\n",
    "    trueList = []\n",
    "    predictListDomain = []\n",
    "    predictListNotDomain = []\n",
    "    for label in true:\n",
    "        trueList.append(label)\n",
    "\n",
    "    for label in predict[domainStart:domainStop]:\n",
    "        predictListDomain.append(label)\n",
    "\n",
    "    for label in predict[:domainStart + 1]:\n",
    "        predictListNotDomain.append(label)\n",
    "    for label in predict[domainStop:]:\n",
    "        predictListNotDomain.append(label)\n",
    "\n",
    "    trueList = set(trueList)\n",
    "    trueList = list(trueList)\n",
    "    predictListDomain = set(predictListDomain)\n",
    "    predictListDomain = list(predictListDomain)\n",
    "    predictListNotDomain = set(predictListNotDomain)\n",
    "    predictListNotDomain = list(predictListNotDomain)\n",
    "    \n",
    "    for i in range(len(trueList)):\n",
    "        trueList[i] = int(trueList[i])\n",
    "    for i in range(len(predictListDomain)):\n",
    "        predictListDomain[i] = int(predictListDomain[i])\n",
    "    for i in range(len(predictListNotDomain)):\n",
    "        predictListNotDomain[i] = int(predictListNotDomain[i])\n",
    "        \n",
    "    \n",
    "        \n",
    "    return (trueList, predictListDomain, predictListNotDomain)\n",
    "\n",
    "def evaluate_positions(true, predict):\n",
    "    switch1 = True\n",
    "    switch2 = True\n",
    "    predStart = None\n",
    "    trueStart = None\n",
    "    predStop = None\n",
    "    trueStop = None\n",
    "    counter = 0\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(len(true)):\n",
    "\n",
    "        if true[i] != 0 and switch2:\n",
    "            switch2 = False\n",
    "            trueStart = i\n",
    "#                 print(f\"REAL START POSITION {trueStart}\")\n",
    "\n",
    "        if predict[i] != 0 and switch1:\n",
    "            switch1 = False\n",
    "            predStart = i\n",
    "#                 print(f\"PRED START POSITION {predStart}\")\n",
    "\n",
    "        if true[i] == 0 and switch2 == False:\n",
    "            switch2 = None\n",
    "            trueStop = i - 1\n",
    "#                 print(f\"REAL STOP POSITION {trueStop}\")\n",
    "\n",
    "        if predict[i] == 0 and switch1 == False:\n",
    "            switch1 = None\n",
    "            predStop = i - 1\n",
    "#                 print(f\"PRED STOP POSITION {predStop}\")\n",
    "\n",
    "#             if predict[i] != true[i]:\n",
    "#                 print(f\"{counter}) {true[i]}|=|{predict[i]} >> NOT\")\n",
    "#             else:\n",
    "#                 print(f\"{counter}) {true[i]}|=|{predict[i]}\")\n",
    "        counter += 1\n",
    "\n",
    "    if switch2 == False and trueStop == None:\n",
    "        trueStop = len(true)\n",
    "\n",
    "    if switch1 == False and predStop == None:\n",
    "        predStop = len(predict)\n",
    "\n",
    "    if trueStart == None and switch2 == True:\n",
    "        print(\"HAAAAAAAAAAAAAAAAAAA NO DOMAIN\")\n",
    "        print(trueStart, trueStop, predStart, predStop)\n",
    "\n",
    "    return (trueStart, trueStop, predStart, predStop)\n",
    "#==================================\n",
    "def metric_extractor(strat_test, pipeline):\n",
    "    meatpipe = pipeline\n",
    "    #parses through each sample and adds metrics to dict which is then appended to a list\n",
    "    masterList = []\n",
    "    sampleCounter = 0\n",
    "\n",
    "    for i in range(len(strat_test)):\n",
    "        labelDict = {}\n",
    "        sampleDict = {}\n",
    "        pred_labels = []\n",
    "        tempList = meatpipe(strat_test.iloc[i][\"sequence\"], max_length=512)\n",
    "        label = strat_test.iloc[i][\"labels\"]\n",
    "        sequence = strat_test.iloc[i][\"sequence\"]\n",
    "        start = strat_test.iloc[i][\"start\"]\n",
    "        stop = strat_test.iloc[i][\"stop\"]\n",
    "        true_labels = labelLabel(label, sequence, start, stop)\n",
    "\n",
    "\n",
    "        for aminoDict in tempList:\n",
    "            labelNum = aminoDict[\"entity\"]\n",
    "            labelNum = int(labelNum[6:])\n",
    "            pred_labels.append(labelNum)\n",
    "\n",
    "        positions = evaluate_positions(true_labels, pred_labels)\n",
    "        trueStart = positions[0]\n",
    "        trueStop = positions[1]\n",
    "        predStart = positions[2]\n",
    "        predStop = positions[3]\n",
    "\n",
    "        eval_metrics = domainAcc(true_labels, pred_labels)\n",
    "        eval_start = eval_metrics[0]\n",
    "        eval_stop = eval_metrics[1]\n",
    "        domainLabelAcc = eval_metrics[2]\n",
    "        notDomainLabelAcc = eval_metrics[3]\n",
    "        fullDomainAcc = eval_metrics[4]\n",
    "\n",
    "        domainStart = eval_start - 1\n",
    "        domainStop = eval_stop\n",
    "\n",
    "        items = matching_labels(true_labels, pred_labels, domainStart, domainStop)\n",
    "        trueItems = items[0]\n",
    "        predDomItems = items[1]\n",
    "        predNotDomItems = items[2]\n",
    "\n",
    "        if notDomainLabelAcc < 0:\n",
    "            print(\"NEGATIVE VALUE\")\n",
    "    #         print(f\"{notDomainLabelAcc} = {eval_metrics[5]} / {eval_metrics[6]}\")\n",
    "    #         print(start, stop)\n",
    "    #         print(trueStart, trueStop)\n",
    "    #         print(label)\n",
    "\n",
    "        sampleDict[\"fullDomain_accuracy\"] = float(fullDomainAcc)\n",
    "        sampleDict[\"domain_accuracy\"] = float(domainLabelAcc)\n",
    "        sampleDict[\"notDomain_accuracy\"] = float(notDomainLabelAcc)\n",
    "        sampleDict[\"true_labels\"] = list(trueItems)\n",
    "        sampleDict[\"predDomain_labels\"] = list(predDomItems)\n",
    "        sampleDict[\"predNotDomain_labels\"] = list(predNotDomItems)\n",
    "        sampleDict[\"trueStart\"] = trueStart\n",
    "        sampleDict[\"trueStop\"] = trueStop\n",
    "        sampleDict[\"predStart\"] = float(predStart)\n",
    "        sampleDict[\"predStop\"] = float(predStop)\n",
    "\n",
    "        for i in trueItems:\n",
    "            if i != 0:\n",
    "                attentionLabel = str(i)\n",
    "\n",
    "\n",
    "        labelDict[attentionLabel] = sampleDict\n",
    "\n",
    "        masterList.append(labelDict)\n",
    "\n",
    "        sampleCounter += 1\n",
    "        if sampleCounter % 100 == 0:\n",
    "            print(f\"{sampleCounter} / {len(strat_test)}\")\n",
    "    #     print(sampleDict)\n",
    "    #     print(\"=============================\")\n",
    "    return masterList\n",
    "\n",
    "def group_labels(masterList):\n",
    "    #parses through masterList to group the metrics of each sample by label for easier evaluation\n",
    "    masterEvalDict = {}\n",
    "    sampleCounter = 0\n",
    "    for labelDictLevel in masterList:\n",
    "        lab = list(labelDictLevel.keys())\n",
    "        lab = lab[0]\n",
    "        labelDict = labelDictLevel[lab]\n",
    "        if lab not in masterEvalDict:\n",
    "            newDict = {\n",
    "                \"fullDomain_accuracy\": [labelDict[\"fullDomain_accuracy\"]],\n",
    "                \"domain_accuracy\": [labelDict[\"domain_accuracy\"]],\n",
    "                \"notDomain_accuracy\": [labelDict[\"notDomain_accuracy\"]],\n",
    "                \"true_labels\": list(labelDict[\"true_labels\"]),\n",
    "                \"predDomain_labels\": list(labelDict[\"predDomain_labels\"]),\n",
    "                \"predNotDomain_labels\": list(labelDict[\"predNotDomain_labels\"])\n",
    "            }\n",
    "            if labelDict[\"trueStart\"] == labelDict[\"predStart\"]:\n",
    "                newDict[\"start\"] = [1]\n",
    "            else:\n",
    "                newDict[\"start\"] = [0]\n",
    "\n",
    "            if labelDict[\"trueStop\"] == labelDict[\"predStop\"]:\n",
    "                newDict[\"stop\"] = [1]\n",
    "            else:\n",
    "                newDict[\"stop\"] = [0]\n",
    "\n",
    "            masterEvalDict[lab] = newDict\n",
    "        else:\n",
    "            innerLabelDict = masterEvalDict[lab]\n",
    "            innerLabelDict[\"fullDomain_accuracy\"].append(labelDict[\"fullDomain_accuracy\"])\n",
    "            innerLabelDict[\"domain_accuracy\"].append(labelDict[\"domain_accuracy\"])\n",
    "            innerLabelDict[\"notDomain_accuracy\"].append(labelDict[\"notDomain_accuracy\"])\n",
    "\n",
    "            innerLabelDict[\"true_labels\"] = list(innerLabelDict[\"true_labels\"])\n",
    "            innerLabelDict[\"true_labels\"] += labelDict[\"true_labels\"]\n",
    "            innerLabelDict[\"true_labels\"] = set(innerLabelDict[\"true_labels\"])\n",
    "            innerLabelDict[\"true_labels\"] = list(innerLabelDict[\"true_labels\"])\n",
    "\n",
    "            innerLabelDict[\"predDomain_labels\"] = list(innerLabelDict[\"predDomain_labels\"])\n",
    "            innerLabelDict[\"predDomain_labels\"] += labelDict[\"predDomain_labels\"]\n",
    "            innerLabelDict[\"predDomain_labels\"] = set(innerLabelDict[\"predDomain_labels\"])\n",
    "            innerLabelDict[\"predDomain_labels\"] = list(innerLabelDict[\"predDomain_labels\"])\n",
    "\n",
    "\n",
    "            innerLabelDict[\"predNotDomain_labels\"] = list(innerLabelDict[\"predNotDomain_labels\"])\n",
    "            innerLabelDict[\"predNotDomain_labels\"] += labelDict[\"predNotDomain_labels\"]\n",
    "            innerLabelDict[\"predNotDomain_labels\"] = set(innerLabelDict[\"predNotDomain_labels\"])\n",
    "            innerLabelDict[\"predNotDomain_labels\"] = list(innerLabelDict[\"predNotDomain_labels\"])\n",
    "\n",
    "            if labelDict[\"trueStart\"] == labelDict[\"predStart\"]:\n",
    "                innerLabelDict[\"start\"].append(1)\n",
    "            else:\n",
    "                innerLabelDict[\"start\"].append(0)\n",
    "\n",
    "            if labelDict[\"trueStop\"] == labelDict[\"predStop\"]:\n",
    "                innerLabelDict[\"stop\"].append(1)\n",
    "            else:\n",
    "                innerLabelDict[\"stop\"].append(0)\n",
    "\n",
    "        sampleCounter += 1\n",
    "        if sampleCounter % 100 == 0:\n",
    "            print(f\"{sampleCounter} / {len(masterList)}\")\n",
    "            \n",
    "    return masterEvalDict\n",
    "\n",
    "def average_metrics(masterEvalDict):    \n",
    "    #compiles the metrics for each label into a label average\n",
    "    resultDict = {}\n",
    "    sampleCounter = 0\n",
    "\n",
    "\n",
    "    for eachLabel in masterEvalDict:\n",
    "        innerLabDict = {}\n",
    "\n",
    "        metricData = masterEvalDict[eachLabel]\n",
    "        fullDom_acc = sum(metricData[\"fullDomain_accuracy\"]) / len(metricData[\"fullDomain_accuracy\"])\n",
    "        dom_acc = sum(metricData[\"domain_accuracy\"]) / len(metricData[\"domain_accuracy\"])\n",
    "        notDom_acc = sum(metricData[\"notDomain_accuracy\"]) / len(metricData[\"notDomain_accuracy\"])\n",
    "\n",
    "        true_lab = list(metricData[\"true_labels\"])\n",
    "        predDomLab = list(metricData[\"predDomain_labels\"])\n",
    "        predNotdomLab = list(metricData[\"predNotDomain_labels\"])\n",
    "\n",
    "        start_acc = sum(metricData[\"start\"]) / len(metricData[\"start\"])\n",
    "        stop_acc = sum(metricData[\"stop\"]) / len(metricData[\"stop\"])\n",
    "\n",
    "        innerLabDict[\"fullDomain_accuracy\"] = fullDom_acc\n",
    "        innerLabDict[\"domain_accuracy\"] = dom_acc\n",
    "        innerLabDict[\"notDomain_accuracy\"] = notDom_acc\n",
    "\n",
    "        innerLabDict[\"true_labels\"] = true_lab\n",
    "        innerLabDict[\"pred_domain_labels\"] = predDomLab\n",
    "        innerLabDict[\"pred_notDomain_labels\"] = predNotdomLab\n",
    "\n",
    "        innerLabDict[\"start_acc\"] = start_acc\n",
    "        innerLabDict[\"stop_acc\"] = stop_acc\n",
    "\n",
    "        resultDict[eachLabel] = innerLabDict\n",
    "\n",
    "        sampleCounter += 1\n",
    "        print(f\"{sampleCounter} / {len(masterEvalDict)}\")\n",
    "    return resultDict\n",
    "\n",
    "def present_labels(csvPath, sample_num):\n",
    "    \n",
    "    df = pd.read_csv(csvPath)\n",
    "\n",
    "    print(df.columns)\n",
    "    df = df.drop([\"Unnamed: 0\"], axis=1)\n",
    "    print(df.columns)\n",
    "\n",
    "    #dropped duplicate rows\n",
    "    print(len(df))\n",
    "    df.drop_duplicates(subset=[\"sequence\", \"labels\", \"start\", \"stop\"], inplace=True) #if we removed \"start\" we would have 618024 sequences instead, one sequence is exactly the same with just a different start site\n",
    "    print(len(df))\n",
    "    df.reset_index(inplace=True)\n",
    "    df = df.drop([\"index\"], axis=1)\n",
    "    df.tail()\n",
    "\n",
    "    #shows the frequency of each label after removing duplicates\n",
    "    frequencyDictAfter = {}\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if row[\"labels\"] not in frequencyDictAfter:\n",
    "            frequencyDictAfter[row[\"labels\"]] = 1\n",
    "        else:\n",
    "            frequencyDictAfter[row[\"labels\"]] += 1\n",
    "\n",
    "    # counter = 0\n",
    "    # for i in frequencyDictAfter:\n",
    "    #     print(i, frequencyDictAfter[i])\n",
    "    #     counter += 1\n",
    "    # print(counter)\n",
    "\n",
    "    #list out labels with less than 100 samples and remove\n",
    "    dropList = [] #list of labels to drop as they contain less than 100 samples\n",
    "    numToDrop = []\n",
    "    amount  = 0\n",
    "    for i in frequencyDictAfter:\n",
    "        if frequencyDictAfter[i] < sample_num:\n",
    "            #print(i, frequencyDict[i])\n",
    "            dropList.append(i)\n",
    "            numToDrop.append(frequencyDictAfter[i])\n",
    "            amount += 1\n",
    "    print(len(dropList))\n",
    "    dropList.remove(\"AA_not_domain\")\n",
    "    print(len(dropList))\n",
    "    print(\"=====\")\n",
    "    print(len(numToDrop))\n",
    "    numToDrop.pop(len(numToDrop) - 1)\n",
    "    print(len(numToDrop))\n",
    "\n",
    "    # drops the rows with labels in dropList (removes labels with too few sequences)\n",
    "    print(len(df))\n",
    "    df.set_index(\"labels\", inplace=True)\n",
    "    df.drop(dropList, axis=0, inplace=True)\n",
    "    df.reset_index(inplace=True)\n",
    "    print(len(df))\n",
    "\n",
    "    #shows the frequency of each label after removing sequences with less than 100 samples\n",
    "    frequencyDictAfter = {}\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if row[\"labels\"] not in frequencyDictAfter:\n",
    "            frequencyDictAfter[row[\"labels\"]] = 1\n",
    "        else:\n",
    "            frequencyDictAfter[row[\"labels\"]] += 1\n",
    "\n",
    "    counter = 0\n",
    "    for i in frequencyDictAfter:\n",
    "    #     print(i, frequencyDictAfter[i])\n",
    "        counter += 1\n",
    "    print(f\"number of labels: {counter}\")\n",
    "\n",
    "    #encode labels and print a list of them with corresponding number label (goes alphabetically)\n",
    "\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(df[\"labels\"])\n",
    "\n",
    "    printList = []\n",
    "    t = list(le.classes_)\n",
    "    n = 0\n",
    "    for i in t:\n",
    "        print(f\"{n}) {i}\")\n",
    "        printList.append(i)\n",
    "        n += 1\n",
    "    print(\"\")\n",
    "    print(printList)\n",
    "    return printList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterList = metric_extractor(strat_test, meatpipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2': {'fullDomain_accuracy': 0.41479099678456594,\n",
       "  'domain_accuracy': 0.4155405405405405,\n",
       "  'notDomain_accuracy': 0.6,\n",
       "  'true_labels': [0, 2],\n",
       "  'predDomain_labels': [0, 2],\n",
       "  'predNotDomain_labels': [0, 2],\n",
       "  'trueStart': 7,\n",
       "  'trueStop': 303,\n",
       "  'predStart': 0.0,\n",
       "  'predStop': 112.0}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masterList[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 / 96420\n"
     ]
    }
   ],
   "source": [
    "masterEvalDict = group_labels(masterList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 39\n",
      "2 / 39\n",
      "3 / 39\n",
      "4 / 39\n",
      "5 / 39\n",
      "6 / 39\n",
      "7 / 39\n",
      "8 / 39\n",
      "9 / 39\n",
      "10 / 39\n",
      "11 / 39\n",
      "12 / 39\n",
      "13 / 39\n",
      "14 / 39\n",
      "15 / 39\n",
      "16 / 39\n",
      "17 / 39\n",
      "18 / 39\n",
      "19 / 39\n",
      "20 / 39\n",
      "21 / 39\n",
      "22 / 39\n",
      "23 / 39\n",
      "24 / 39\n",
      "25 / 39\n",
      "26 / 39\n",
      "27 / 39\n",
      "28 / 39\n",
      "29 / 39\n",
      "30 / 39\n",
      "31 / 39\n",
      "32 / 39\n",
      "33 / 39\n",
      "34 / 39\n",
      "35 / 39\n",
      "36 / 39\n",
      "37 / 39\n",
      "38 / 39\n",
      "39 / 39\n"
     ]
    }
   ],
   "source": [
    "resultDict = average_metrics(masterEvalDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reload from csv, drop index column\n",
    "csvName = \"parsed_pullin_sequences>100_whiteSpace_withAA_not_domain_uneven.csv\"\n",
    "csvPath = f\"/mnt/storage/grid/home/eric/hmm2bert/pullin_parsed_data/{csvName}\"\n",
    "sample_num = 1000\n",
    "\n",
    "print(present_labels(csvPath, sample_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'resultDict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dbdce4b16352>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinalZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresultDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/mnt/storage/grid/home/eric/hmm2bert/pullin_parsed_data/pullin>1000_TESTING\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinalZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'resultDict' is not defined"
     ]
    }
   ],
   "source": [
    "finalZ = resultDict\n",
    "with open(\"/mnt/storage/grid/home/eric/hmm2bert/pullin_parsed_data/pullin>1000_TESTING.json\", \"w\") as file:\n",
    "    json.dump(finalZ, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnabert",
   "language": "python",
   "name": "dnabert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
